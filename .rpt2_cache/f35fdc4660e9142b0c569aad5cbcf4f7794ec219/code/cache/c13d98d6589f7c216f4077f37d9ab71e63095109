{"code":"import rules from './rules';\r\nvar tokenise = function (str, tokens) {\r\n    if (tokens === void 0) { tokens = []; }\r\n    // Look for a matching rule\r\n    var matched = rules.some(function (rule) {\r\n        var match = str.match(rule.pattern);\r\n        if (!match) {\r\n            return false;\r\n        }\r\n        tokens.push({\r\n            type: rule.name,\r\n            match: match[0],\r\n            val: match.slice(1, 2),\r\n            otherVal: match.slice(2),\r\n            regex: rule.regex instanceof Function ? rule.regex(match) : rule.regex\r\n        });\r\n        if (match[0].length < str.length) {\r\n            tokens = tokenise(str.substr(match[0].length), tokens);\r\n        }\r\n        return true;\r\n    });\r\n    // If no rules matched, throw an error (possible malformed path)\r\n    if (!matched) {\r\n        throw new Error(\"Could not parse path '\" + str + \"'\");\r\n    }\r\n    return tokens;\r\n};\r\nexport default tokenise;\r\n","map":{"mappings":""},"dts":{"name":"/Users/thomasroch/Development/path-parser/typings/tokeniser.d.ts","text":"export interface IToken {\r\n    type: string;\r\n    match: string;\r\n    val: any;\r\n    otherVal: any;\r\n    regex: RegExp;\r\n}\r\ndeclare const tokenise: (str: string, tokens?: IToken[]) => IToken[];\r\nexport default tokenise;\r\n"}}
